# üöÄ SISTEMA DE CLASIFICACI√ìN PROFESIONAL CON PyTorch

Entrenamiento de Redes Neuronales Profundas para Clasificaci√≥n de Datos

---

## üìã Descripci√≥n General

Este proyecto implementa un **sistema profesional de clasificaci√≥n** basado en redes neuronales artificiales (MLP - Multi-Layer Perceptron) utilizando PyTorch. Permite entrenar modelos de deep learning con tus propios datos, evaluarlos con m√©tricas est√°ndar de la industria y hacer predicciones sobre nuevos datos.

### Caracter√≠sticas Principales

‚úÖ **Arquitectura Flexible**: Capa ocultas personalizables
‚úÖ **Regularizaci√≥n Avanzada**: BatchNorm, Dropout y Early Stopping
‚úÖ **Preprocesamiento Autom√°tico**: Normalizaci√≥n y codificaci√≥n de categor√≠as
‚úÖ **Visualizaciones Profesionales**: Matrices de confusi√≥n e historial de entrenamiento
‚úÖ **Persistencia de Modelos**: Guarda y carga modelos entrenados
‚úÖ **Soporte GPU/CPU**: Detecci√≥n autom√°tica de dispositivo
‚úÖ **M√©tricas Completas**: Accuracy, F1-Score, Reporte de clasificaci√≥n

---

## üîß Requisitos

- **Python**: 3.10 (64-bit) ‚ö†Ô∏è *Importante: Se requiere Python 64-bit*
- **Sistema Operativo**: Windows, macOS o Linux

### Dependencias Principales

```
torch==2.0.1+cpu
pandas==2.3.3
numpy==2.2.6
scikit-learn==1.7.2
matplotlib==3.10.7
seaborn==0.13.2
joblib==1.5.2
```

---

## üì¶ Instalaci√≥n

### 1. Crear Entorno Virtual con Python 3.10

```powershell
# En Windows PowerShell
python -m venv venv_torch
```

### 2. Activar el Entorno Virtual

```powershell
# En Windows PowerShell
.\venv_torch\Scripts\Activate.ps1
```

### 3. Instalar Dependencias

```powershell
pip install torch==2.0.1+cpu pandas==2.3.3 numpy==2.2.6 scikit-learn==1.7.2 matplotlib==3.10.7 seaborn==0.13.2 joblib==1.5.2
```

---

## üìÇ Estructura del Proyecto

```
proyectos 2/
‚îú‚îÄ‚îÄ clasificador.py              # Archivo principal con todas las clases
‚îú‚îÄ‚îÄ clasificador_profesional.py   # Copia del c√≥digo fuente
‚îú‚îÄ‚îÄ venv_torch/                   # Entorno virtual (NO incluir en repositorio)
‚îú‚îÄ‚îÄ modelos_guardados/            # Modelos entrenados (se crea autom√°ticamente)
‚îú‚îÄ‚îÄ resultados/                   # Gr√°ficas y visualizaciones (se crea autom√°ticamente)
‚îî‚îÄ‚îÄ README.md                     # Este archivo
```

---

## üèóÔ∏è Estructura del C√≥digo

### Clase `Config`
Gestiona todos los par√°metros de configuraci√≥n del modelo:

```python
class Config:
    # Arquitectura
    hidden_layers = [64, 64]      # Neuronas en capas ocultas
    dropout_rate = 0.2             # Tasa de dropout
    
    # Entrenamiento
    epochs = 50                    # √âpocas de entrenamiento
    learning_rate = 0.001          # Tasa de aprendizaje
    batch_size = 32                # Tama√±o del lote
    validation_split = 0.2         # % de datos para validaci√≥n
```

### Clase `FlexibleMLP`
Red Neuronal Multi-Capa con arquitectura personalizable:

- **Capas Din√°micas**: Se adapta a cualquier n√∫mero de caracter√≠sticas
- **BatchNormalization**: Normaliza activaciones entre capas
- **Activaci√≥n ReLU**: Introducida entre capas ocultas
- **Dropout**: Regularizaci√≥n para evitar sobreajuste

```
Entrada ‚Üí Linear ‚Üí BatchNorm1d ‚Üí ReLU ‚Üí Dropout ‚Üí 
          Linear ‚Üí BatchNorm1d ‚Üí ReLU ‚Üí Dropout ‚Üí 
          Linear (salida)
```

### Clase `ClasificadorProfesional`
Orquestador principal que maneja todo el proceso ML:

#### M√©todos Principales:

| M√©todo | Descripci√≥n |
|--------|-------------|
| `cargar_datos_csv()` | Carga datos desde archivo CSV |
| `preparar_datos()` | Normaliza y divide datos (80/20) |
| `crear_modelo()` | Construye la red neuronal |
| `entrenar()` | Entrena el modelo con early stopping |
| `evaluar()` | Calcula m√©tricas y genera visualizaciones |
| `plot_historial()` | Grafica p√©rdida y accuracy |
| `guardar_modelo()` | Persiste el modelo entrenado |
| `cargar_modelo()` | Carga un modelo previamente guardado |
| `predecir()` | Hace predicciones en nuevos datos |

---

## üíª Ejemplos de Uso

### Opci√≥n 1: Ejecutar con Datos Sint√©ticos (Demo)

```python
from clasificador import ClasificadorProfesional, Config, ejemplo_uso_datos_sinteticos

# Ejecuta la demostraci√≥n completa
ejemplo_uso_datos_sinteticos()
```

**Salida esperada**:
- Crea 2000 datos sint√©ticos con 20 caracter√≠sticas y 3 clases
- Entrena la red durante hasta 50 √©pocas
- Muestra m√©tricas: Accuracy, F1-Score, Matriz de Confusi√≥n
- Genera gr√°ficas en carpeta `resultados/`
- Guarda el modelo en carpeta `modelos_guardados/`

---

### Opci√≥n 2: Usar con Tus Propios Datos CSV

#### Paso 1: Preparar el CSV
Tu archivo CSV debe tener:
- Una columna con la **variable objetivo** (lo que quieres predecir)
- Las dem√°s columnas como **caracter√≠sticas** (features)

Ejemplo `datos.csv`:
```
feature1,feature2,feature3,...,target
1.2,0.5,2.1,...,A
2.1,1.3,0.8,...,B
0.9,2.2,1.5,...,A
```

#### Paso 2: Entrenar el Modelo

```python
from clasificador import ClasificadorProfesional, Config
import numpy as np

# Inicializa el clasificador
config = Config()
config.epochs = 100          # Puedes personalizar par√°metros
config.learning_rate = 0.0005

clasificador = ClasificadorProfesional(config)

# Carga los datos
X, y = clasificador.cargar_datos_csv(
    ruta_csv="ruta/a/tu/datos.csv",
    columna_objetivo="nombre_columna_objetivo",
    columnas_excluir=["id", "nombre"]  # Opcional: columnas a ignorar
)

# Prepara los datos
X_train, X_val, y_train, y_val = clasificador.preparar_datos(X, y)

# Crea el modelo
clasificador.crear_modelo(
    input_size=X.shape[1],
    output_size=len(np.unique(y))
)

# Entrena
clasificador.entrenar(X_train, y_train, X_val, y_val)

# Eval√∫a
accuracy, f1, cm = clasificador.evaluar(X_val, y_val)

# Guarda visualizaciones
clasificador.plot_historial()

# Guarda el modelo
ruta_modelo = clasificador.guardar_modelo("mi_modelo")
```

---

### Opci√≥n 3: Hacer Predicciones con Modelo Guardado

```python
from clasificador import ClasificadorProfesional

# Carga un modelo guardado
clasificador = ClasificadorProfesional()
clasificador.cargar_modelo("modelos_guardados/mi_modelo_20251211_120000.pkl")

# Haz predicciones
X_nuevo = [[1.2, 0.5, 2.1, ...]]  # Array con nuevos datos
predicciones, probabilidades = clasificador.predecir(X_nuevo)

print(f"Predicci√≥n: {predicciones}")
print(f"Probabilidades: {probabilidades}")
```

---

## üìä Salida del Programa

### Durante Inicializaci√≥n
```
============================================================
üöÄ CLASIFICADOR PROFESIONAL INICIALIZADO
============================================================
üì± Dispositivo: cpu
üìÇ Modelos guardados en: modelos_guardados
üìä Resultados guardados en: resultados
```

### Durante Entrenamiento
```
============================================================
üéØ INICIANDO ENTRENAMIENTO
============================================================

√âpoca 001/050 | Loss Entreno: 1.0923 | Loss Val: 1.0234 | Precisi√≥n Val: 45.32%
√âpoca 005/050 | Loss Entreno: 0.6234 | Loss Val: 0.5892 | Precisi√≥n Val: 78.15%
...
```

### Despu√©s de Entrenar
```
============================================================
üìä EVALUACI√ìN DEL MODELO
============================================================

üéØ Precisi√≥n (Accuracy): 87.50%
üìà F1-Score: 0.8645

üìã REPORTE DETALLADO POR CLASE:
---
              precision    recall  f1-score   support
       Clase A     0.8900   0.8700   0.8800       120
       Clase B     0.8400   0.8600   0.8500       100
...
```

---

## üîç Par√°metros de Configuraci√≥n

### Arquitectura de la Red

| Par√°metro | Rango Recomendado | Efecto |
|-----------|-------------------|--------|
| `hidden_layers` | `[32, 64, 128]` | Lista de neuronas por capa |
| `dropout_rate` | `0.1 - 0.5` | Mayor = m√°s regularizaci√≥n |

### Entrenamiento

| Par√°metro | Rango Recomendado | Efecto |
|-----------|-------------------|--------|
| `epochs` | `50 - 200` | M√°s √©pocas = m√°s tiempo pero mejor aprendizaje |
| `learning_rate` | `0.0001 - 0.01` | M√°s alto = aprendizaje m√°s r√°pido pero inestable |
| `batch_size` | `16 - 128` | M√°s grande = m√°s memoria pero m√°s r√°pido |

---

## üéØ T√©cnicas Avanzadas Implementadas

### Early Stopping
Detiene el entrenamiento si no hay mejora durante 10 √©pocas consecutivas:
```python
if patience_counter >= 10:
    print("‚ö† Early stopping activado")
    break
```

### Learning Rate Scheduler
Reduce la tasa de aprendizaje si la p√©rdida no mejora:
```python
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5
)
```

### Model Checkpointing
Guarda autom√°ticamente el mejor modelo durante el entrenamiento:
```python
if val_loss < best_val_loss:
    self.best_model_state = self.model.state_dict().copy()
```

---

## üìà M√©tricas de Evaluaci√≥n

### Accuracy (Precisi√≥n)
Porcentaje de predicciones correctas:
$$\text{Accuracy} = \frac{\text{Predicciones Correctas}}{\text{Total de Predicciones}}$$

### F1-Score
Promedio ponderado de precisi√≥n y recall:
$$F1 = 2 \times \frac{\text{Precisi√≥n} \times \text{Recall}}{\text{Precisi√≥n} + \text{Recall}}$$

### Matriz de Confusi√≥n
Tabla que muestra verdaderos positivos, falsos positivos, etc.

---

## üêõ Soluci√≥n de Problemas

### Error: `ModuleNotFoundError: No module named 'torch'`

**Soluci√≥n**: Aseg√∫rate de activar el entorno virtual:
```powershell
.\venv_torch\Scripts\Activate.ps1
```

### Error: `Python 32-bit not compatible with PyTorch`

**Soluci√≥n**: Necesitas Python 3.10 64-bit. Desc√°rgalo de [python.org](https://www.python.org/downloads/)

### Error: `RuntimeError: CUDA out of memory`

**Soluci√≥n**: Reduce `batch_size` en la configuraci√≥n o usa CPU en lugar de GPU.

### Las gr√°ficas no se muestran

**Soluci√≥n**: Las gr√°ficas se guardan autom√°ticamente en la carpeta `resultados/`. √Åbrelas con un explorador de archivos.

---

## üìö Conceptos Clave

### MLP (Multi-Layer Perceptron)
Red neuronal feedforward con m√∫ltiples capas ocultas que aprende representaciones complejas.

### BatchNormalization
Normaliza las entradas de cada capa para acelerar el entrenamiento y mejorar la estabilidad.

### Dropout
Desactiva aleatoriamente neuronas durante el entrenamiento para evitar sobreajuste.

### CrossEntropyLoss
Funci√≥n de p√©rdida est√°ndar para problemas de clasificaci√≥n multiclase.

### Adam Optimizer
Optimizador adaptativo que combina ventajas de AdaGrad y RMSprop.

---

## üìÑ Estructura de Archivos Guardados

### Modelos Guardados
```
modelos_guardados/
‚îú‚îÄ‚îÄ modelo_ejemplo_20251211_120000.pkl
‚îú‚îÄ‚îÄ mi_modelo_20251211_150530.pkl
‚îî‚îÄ‚îÄ ...
```

Contienen:
- Pesos del modelo (`model_state_dict`)
- Arquitectura (`model_architecture`)
- Normalizador (`scaler`)
- Codificador de etiquetas (`label_encoder`)
- Historial de entrenamiento (`history`)

### Resultados
```
resultados/
‚îú‚îÄ‚îÄ confusion_matrix_20251211_120000.png
‚îú‚îÄ‚îÄ training_history_20251211_120000.png
‚îî‚îÄ‚îÄ ...
```

---

## üöÄ Pr√≥ximos Pasos

1. **Ajusta los hiperpar√°metros** en la clase `Config` seg√∫n tu dataset
2. **Experimenta con diferentes arquitecturas** modificando `hidden_layers`
3. **Aumenta el volumen de datos** para mejor generalizaci√≥n
4. **Valida con datos nuevos** usando el m√©todo `predecir()`
5. **Guarda modelos prometedores** para reutilizarlos despu√©s

---

## üìû Informaci√≥n de Contacto

**Creado**: Noviembre 2025
**Versi√≥n**: 1.0
**Python**: 3.10+
**PyTorch**: 2.0.1

---

## üìñ Referencias

- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [scikit-learn Guide](https://scikit-learn.org/stable/)
- [Neural Networks Basics](https://en.wikipedia.org/wiki/Artificial_neural_network)

---

**¬°Disfruta entrenando tus modelos!** üéâ
